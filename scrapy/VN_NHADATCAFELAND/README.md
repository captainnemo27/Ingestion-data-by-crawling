# Crawling

## 1. Objective 

- Download and parse data from **NHADATCAFELAND** site using bash script or scrapy & awk.

## 2. How does it work ?  
**A Script structure described as below:**
-  download_site.sh : this is main script that calls other scripts using **SCRAPY**
-  download_site_bashscript.sh: this is main script that calls other scripts using **BASHSCRIPT**
-  awk:
    - put_html_to_tab.awk: convert data from list mode pages to **extract.tab**
    - put_tab_to_db.awk: convert data from **extract.tab** to insert SQL script  
    - get_ads_details.awk: convert data from ads pages to update SQL script
    - get_list_city.awk: generate list of possible cities and districts can download
    - get_list_ward.awk: generate list of possible ward can download
-  bash:
    - parsing_insert.sh: run put_tab_to_db.awk to create **ads_insert.sql** for further import
    - parsing_update.sh: run get_ads_details.awk to create **ads_update.sql** for further import
-  python:  
    - download folder: contains scrapy scripts to perform crawling .html files
    - nhadatcafeland_helper.py: defined utiliy functions
-  requirement.txt: contains all libraries with its version

### Step 0: Create following folders
-  ALL/: contains detail ads .html files.
-  DELTA/: 
    - ads_insert.sql: to insert the new ads to database
    - ads_update.sql: to update information about the new ads to database
    - extract_backup.tab: ads information getting from list pages
    - extract.tab (daily download): extract.tab after compare
    - download_list.temp: list of possible page can download, every line includes url, number of ads, location and type. Example: `https://nhadat.cafeland.vn/nha-dat-ban-tai-phuong-11-quan-5/	54 phuong-11-quan-5 nha-dat-ban`
    - status_ok: contains ``ok`` content to indicate that the program runs completely
- LIST_MODE/: contains list page .html files
- TEMP/: 
    - .*html: list of page to get location
    - nha-dat-ban_country_extract.tab: list city for sale: number of ads, link and name
    - nha-dat-ban_city_extract.tab: list district for sale: number of ads, link and name
    - nha-dat-ban_district_extract.tab: list ward for sale: number of ads, link and name
    - nha-dat-cho-thue_country_extract.tab: list city for lease: number of ads, link and name
    - nha-dat-cho-thue_city_extract.tab: list district for lease: number of ads, link and name
    - nha-dat-cho-thue_district_extract.tab: list ward for lease: number of ads, link and name
- LOG/:
    - scrapy_site.log: scrapy log when download list pages
    - scrapy_ads.log: scrapy log when download ads files
After running the `download_site.sh`, the script will generate the **three** .txt files:
- date.txt: contains date string (created date folder by command line or setting default)
- option_file.txt: contains has two values 1 or 0. If value = 1, download 2 single category for testing; value = 0 then download full
- max_page_number.txt: contains has two values: 2 or maximum page number of the site.

###  Step 1: Download list pages and save .html files in LIST_MODE folder
-  There are some main problems when download from this site:
    - This site block IP although using company IP `http://172.16.1.11:3128`
    - This site limit **385** page of view
    - This site fake total number of ads. For example, the sum of total ads in district 1 is 8425 but it show 11521. The result number is only true if in the smallest filter by using ward condition
-  Solution: find page download based on location. For every city, generate its district. For every districts, generate its wards. The number of ads in every ward is the exact number. Rotation IP generating by tor, `172.16.0.190:16379`, can solve block IP.

1/ Using bashscript
-  The `download_list_mode` in download_site_bashscript.sh will begin and save all html page files in LIST_MODE folder.

2/ Using scrapy
-   The `scrapy crawl download_site` in download_site.sh calls **download/spider.py**. This download/spider.py (download_site) will perform downloading all categories of list pages and store html files in LIST_MODE folder.
- These scripts **(items.py, middlewares.py, pipelines.py, settings.py, scrapy.cfg)** generated by default by Scrapy framework and used to call the object class in the spider.py. Note that THESE FILES ARE OBLIGATORY and we CAN'T DELETE THEM and DON'T NEED TO CARE THEM because the spider.py is a main script to handle all requests.

### Step 2: Download detail pages and save .html files in ALL folder
- Parsing the every file in LIST_MODE/FOR_LEASE and LIST_MODE/FOR_SALE folder to create extract.tab
- Copy extract.tab to create extract_backup.tab
- If fully download, the script will generate list_need_to_download from full extract.tab. If daily download, the script will generate list_need_to_download from extract_update.tab, which created by comparing current extract.tab with its previous extract_backup.tab in the lastest folder.

1/ Using bashscript
- The `download_detail_mode` will use list_id.txt to perform to download details pages and store html files in ALL/ folder.

2/ Using scrapy
-  The `scrapy crawl download_ads` in download_site.sh calls **download/spider.py** to download details pages and store .html files in ALL/ folder. The class loads id, url from DELTA/list_id.txt to download these detail pages.

### Step 3: Parsing and insert data into the database
-  Parsing:  
    - Parsing list mode: for every line in extract.tab, the script bash/parsing_insert.sh will create **ads_insert.sql**
    - Parsing detail mode: for every html file in all folder, the script bash/parsing_update.sh will create **ads_update.sql**

-  Import: connect to mysql and import **ads_insert.sql** and **ads_update.sql** to the database 

## 3. How to run
**3.1/ Prerequisites**
- Libraries:
    - urllib3==1.25.8
    - Scrapy==2.2.1
    - scrapy-fake-useragent==1.4.4               
    - scrapy-user-agents==0.1.1
    - beautifulsoup4==4.9.1
- Install all libraries in requirements.txt: ``` pip3 install -r requirements.txt ```

**3.2/ How to run the script**

  ```
  * Options: 
     -x = debug
     -y = test mode
     -i = import SQL to database
     -r = Download Ads from LIST_MODE folder (don't re-download list pages)
     -d YYYYmmdd = date of download.
     -D = daily mode
  ```
    a) Test mode:    
        ./download_site.sh -zVN_NHADATCAFELAND -d{_date_store_} -x -y > log{_date_store_} 2>&1 &
        Example: ./download_site.sh -zVN_NHADATCAFELAND -d20200825 -x -y > log20200825 2>&1 &  

    b) Full mode:      
        ./download_site.sh -zVN_NHADATCAFELAND -d{_date_store_} -x > log{_date_store_} 2>&1 &   
        Example: we specific {_date_store_} is 20200825
        ./download_site.sh -zVN_NHADATCAFELAND -d20200825 -x > log20200825 2>&1 & 

    c) Daily mode:
        ./download_site.sh -zVN_NHADATCAFELAND -d{_date_store_} -x -D > log{_date_store_} 2>&1 &
        Example: ./download_site.sh -zVN_NHADATCAFELAND -d20200825 -x -D > log20200825 2>&1 &  
        
**Note:**
-   Download by scrapy is set by default. If download by using bash script, change `download_site.sh` to `download_site_bashscript.sh`
-   -x option must be call first others
-   If there is no {_date_store_}, {_date_store_} = today by default. Example: If today is 25/08/2020, if `./download_site.sh -zVN_NHADATCAFELAND -x > log20200825 2>&1 &` is run, folder will be 20200825
-   If there is no option -y, the script will run full mode by default
-   The script will parse data automatically or manually by calling `bash/ads_insert.sql {_date_store_}`  or `bash/ads_update.sql {_date_store_}` or `bash/ads_update_tel.sql {_date_store_}`
-   If you import data into database, you will use -i options.      
-   If there is a test mode, only 2 pages in 2 wards are created